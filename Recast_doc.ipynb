{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzJIl9Yk+hSaKDbCWkoaOO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricaAndreose/enoam_doc/blob/main/Recast_doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO3b_gLb2eDu"
      },
      "outputs": [],
      "source": [
        "import re, unicodedata, unidecode\n",
        "from bs4 import BeautifulSoup\n",
        "from qwikidata.entity import WikidataItem\n",
        "from qwikidata.linked_data_interface import get_entity_dict_from_api"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qui stiamo importando le seguenti librerie:\n",
        "\n",
        "\n",
        "*   **re**: Per lavorare con le espressioni regolari.\n",
        "*   **unicodedata**: Per lavorare con i dati Unicode.\n",
        "*   **unidecode**: Per traslitterare i caratteri Unicode in caratteri ASCII.\n",
        "*   **BeautifulSoup**: Per analizzare e fare parsing di documenti HTML.\n",
        "*   **WikidataItem**: Per interagire con gli elementi di Wikidata.\n",
        "*   **get_entity_dict_from_api**: Per ottenere i dati di un'entità da Wikidata tramite l'API."
      ],
      "metadata": {
        "id": "m_ucdIeV2j9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONVERT FAKE HEAD AND BODY INTO ACTUAL TAGS\n",
        "def convert_head_body(old_soup, soup):  # it takes in input two BeautifulSoup objects\n",
        "    headfile = old_soup.find('div', id='headFile') # searching for \"div\" elements with \"headFile\" as id\n",
        "    bodyfile = old_soup.find('div', id='bodyFile') # searching for \"div\" elements with \"bodyFile\" as id\n",
        "    if headfile:\n",
        "        soup.head.insert(1, headfile) # Insert headfile into soup's head tag\n",
        "        headfile.unwrap() # Remove headfile from its original position\n",
        "    if bodyfile:\n",
        "        soup.body.insert(1, bodyfile) # Insert bodyfile into soup's body tag\n",
        "        bodyfile.unwrap() # Remove bodyfile from its original position"
      ],
      "metadata": {
        "id": "EsvmPQ0T2-91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funzione `convert_head_body` prende due oggetti BeautifulSoup come input: `old_soup` e `soup`.\n",
        "\n",
        "Cerca all'interno di `old_soup` due elementi div con gli id \"headFile\" e \"bodyFile\". Se questi elementi vengono trovati, vengono inseriti rispettivamente all'interno dei tag `head` e `body` di `soup`, e poi vengono rimossi dalla loro posizione originale in `old_soup` usando il metodo `unwrap()`.\n",
        "\n",
        "In pratica, questa funzione sposta il contenuto dei `div` \"headFile\" e \"bodyFile\" da `old_soup` ai tag `head` e `body` di `soup`, aggiornando la struttura del documento HTML.\n",
        "\n",
        "❓ Dove prende tutte queste cose di input? Vai a vedere su main.py quando lanci \"r\" per recast."
      ],
      "metadata": {
        "id": "Oe-bRQD94oSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INSERT META TAGS\n",
        "def insert_seo_meta(soup, metadata): # takes in input the refined BS object and the metadata from MongoDB database\n",
        "    split_ident = metadata['ident'].split('_') # split the 'ident' field of the metadata stored for every object in MongoDB (ex. 2_2_0_001)\n",
        "\n",
        "    section = '' # create empty string variables\n",
        "    volume = ''\n",
        "    curators = ''\n",
        "    if split_ident[0] == '1':\n",
        "        section = 'Scritti e Discorsi'\n",
        "        if split_ident[1] == '1':\n",
        "            volume = 'Gli anni giovanili (1932-1946)'\n",
        "            curators = 'Gaetano Crociata e Paolo Trionfini'\n",
        "        elif split_ident[1] == '2':\n",
        "            volume = 'Il periodo dossettiano e di Iniziativa democratica (1946-1958)'\n",
        "            if split_ident[2] == '1':\n",
        "                volume = 'Il periodo dossettiano (1946-1951)'\n",
        "            elif split_ident[2] == '2':\n",
        "                volume = 'Iniziativa democratica (1952-1958)'\n",
        "            curators = 'Ugo De Siervo e Enrico Galavotti'\n",
        "        elif split_ident[1] == '3':\n",
        "            volume = 'Il centro-sinistra (1952-1958)'\n",
        "            if split_ident[2] == '1':\n",
        "                volume = 'Segretario della DC (1959-1963)'\n",
        "            elif split_ident[2] == '2':\n",
        "                volume = 'La prima legislatura di centro-sinistra (1964-1968)'\n",
        "            curators = 'Leopoldo Nuti e Paolo Pombeni'\n",
        "        elif split_ident[1] == '4':\n",
        "            volume = 'L’ultima fase (1969-1978)'\n",
        "            if split_ident[2] == '1':\n",
        "                volume = 'Al ministero degli Esteri e all’opposizione nel partito (giugno 1968 – maggio 1973)'\n",
        "            elif split_ident[2] == '2':\n",
        "                volume = 'Il ritorno al centro-sinistra e la “solidarietà nazionale” (giugno 1973 – maggio 1978)'\n",
        "            curators = 'Guido Formigoni e Agostino Giovagnoli'\n",
        "    elif split_ident[0] == '2':\n",
        "        section = 'Opere Giuridiche'\n",
        "        if split_ident[1] == '1':\n",
        "            volume = 'Le prime monografie (1939-1942)'\n",
        "            curators = 'Luciano Eusebi'\n",
        "        elif split_ident[1] == '2':\n",
        "            volume = 'Le dispense di filosofia del diritto (1941-1947)'\n",
        "            curators = 'Nicola Antonetti e Renato Moro'\n",
        "        elif split_ident[1] == '3':\n",
        "            volume = 'Le monografie del dopoguerra (1947-1951)'\n",
        "            curators = 'Marco Pelissero'\n",
        "        elif split_ident[1] == '4':\n",
        "            volume = 'Le lezioni di istituzioni di diritto e procedura penale'\n",
        "            curators = 'Marco Pelissero'"
      ],
      "metadata": {
        "id": "ybLVaj9x5R8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funzione `insert_seo_meta` prende in input un oggetto BeautifulSoup (`soup`) e i metadati (`metadata`) di un documento. Lo scopo di questa funzione è quello di inserire meta tag nel documento HTML rappresentato da soup basandosi sui metadati forniti da MongoDB.\n",
        "\n",
        "Inizialmente, l'identificativo del documento (`ident`) viene estratto dai metadati di MongoDB e suddiviso in parti (viene splittato). Queste parti vengono poi utilizzate per determinare la sezione, il volume e i curatori del documento.\n",
        "\n",
        "La funzione crea nuovi tag meta per la sezione, il volume e i curatori, impostando il loro contenuto in base ai valori determinati in precedenza. Questi tag meta vengono quindi aggiunti all'head del documento HTML.\n",
        "\n",
        "Qui è importante controllare la correttezza dei dati inseriti nel codice (sono già state fatte alcune modifiche in corso d'opera per la correzione di alcuni nomi di ricercatori). Nel caso di introduzione di nuovi sezioni/volumi nell'edizione digitale, vanno aggiunti qui.\n",
        "\n",
        "Ciò che ci crea qui verrà visualizzato concretamente nel sito nella pagina del singolo documento nella sezione \"metadati\" e \"citazione bibliografica\".\n"
      ],
      "metadata": {
        "id": "sLCYxSImoMFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    description = soup.new_tag('meta') #create variable with new meta tag\n",
        "    description['name'] = 'description' #create attribute name\n",
        "    description['content'] = f\"Edizione Nazionale delle Opere di Aldo Moro, {section}, {volume}, 2024\" #create content of the meta tag taking the volum section infos from the variables from before\n",
        "\n",
        "    rights = soup.new_tag('meta')\n",
        "    rights['name'] = 'dcterms.rights'\n",
        "    rights['content'] = 'https://creativecommons.org/licenses/by-nc/4.0'\n",
        "\n",
        "    doi = soup.new_tag('meta')\n",
        "    doi['name'] = 'dcterms.identifier'\n",
        "    doi['content'] = f\"10.48678/unibo/aldomoro{'.'.join(split_ident)}\"\n",
        "\n",
        "    url = soup.new_tag('meta')\n",
        "    url['name'] = 'dcterms.relation'\n",
        "    url['content'] = f\"https://doi.org/{doi['content']}\"\n",
        "\n",
        "    citation = soup.new_tag('meta')\n",
        "    citation['name'] = 'dcterms.bibliographicCitation'\n",
        "    citation['content'] = f\"Moro, Aldo, {metadata['title']}, in Aldo Moro, Edizione Nazionale delle Opere di Aldo Moro, Sezione {split_ident[0]}, {section}, Vol. {split_ident[1]}, {volume}, a cura di {curators}, edizione e nota storico-critica di {metadata['curator']}, Bologna, Università di Bologna, 2024. DOI: https://doi.org/10.48678/unibo/aldomoro{'.'.join(split_ident)}.\"\n",
        "\n",
        "    viewport = soup.new_tag('meta')\n",
        "    viewport['name'] = 'viewport'\n",
        "    viewport['content'] = 'width=device-width, initial -scale=1.0'\n",
        "    charset = soup.new_tag('meta')\n",
        "    charset['charset'] = 'utf-8'\n",
        "\n",
        "    soup.head.insert(0, description) #insert the new tags in the head section of the html\n",
        "    soup.head.insert(0, citation)\n",
        "    soup.head.insert(0, doi)\n",
        "    soup.head.insert(0, url)\n",
        "    soup.head.insert(0, rights)\n",
        "    soup.head.insert(0, viewport)\n",
        "    soup.head.insert(0, charset)\n"
      ],
      "metadata": {
        "id": "sXRDEXIjp6gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qui si creano e si inseriscono i meta tag nell'html finale.\n",
        "\n",
        "`soup.new_tag('meta')`: Crea un nuovo tag meta e lo assegna alla variabile desiderata (description, rights, doi etc.).\n",
        "\n",
        "Viene poi impostato l'attributo `name` e il `content` del tag.\n",
        "\n",
        "Fai attenzione specialmente alla *bibliographic citation*, qui abbiamo cambiato l'anno per allinearci con il periodo in cui abbiamo generato i nuovi documenti caricati nell'edizione.\n"
      ],
      "metadata": {
        "id": "YnprqXPFqoDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE EMPTY TAGS INSIDE HEAD\n",
        "def clean_empty_head(soup):\n",
        "    tags = soup.head.find_all('div') #find all div tag in the head and store them in the tags variable\n",
        "    for tag in tags: #iterate through the tags variable\n",
        "        if not tag.find('meta'): #if the tag does not contain a meta tag\n",
        "            tag.decompose() #remove the tag from the document\n",
        "\n",
        "\n",
        "# REMOVE EMPTY TAGS INSIDE BODY\n",
        "def clean_empty_body(soup):\n",
        "    tags = soup.body.find_all() #find all tags in the body and store them in the tags variable\n",
        "    for tag in tags:\n",
        "        if len(tag.get_text(strip=True)) == 0: #if the tag is empty\n",
        "            tag.decompose() #remove the tag from the document\n",
        "        elif tag.has_attr('class') and tag['class'] == 'no-abstract': #if the tag has the class 'no-abstract'\n",
        "            tag.unwrap()  #remove the tag from the document\n"
      ],
      "metadata": {
        "id": "X9rPZjUbCugs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qui si puliscono il tag `head` e il tag `body` del documento HTML rimuovendo tutti i tag `div` vuoti, (ovvero quelli che non contengono tag `meta`) e nel body quelli con la classe `no-abstract`."
      ],
      "metadata": {
        "id": "Zeyu1IwgM9Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX TITLE AND REMOVE TAGS INSIDE IT\n",
        "def clean_title(soup, spans, metadata):\n",
        "    title = soup.new_tag('title') #create new variable title\n",
        "\n",
        "    # THEIR DOCUMENTS ARE EXCEPTIONS!!!\n",
        "    if metadata['curator'] != 'Caterina Iagnemma' and metadata['curator'] != 'Sofia Confalonieri' and metadata['curator'] != 'Maurizio Cau': #if the curator IS NOT Iagnemma, Confalonieri or Cau\n",
        "\n",
        "        h3 = soup.find('h3')\n",
        "        h2 = soup.find('h2')\n",
        "        h1 = soup.find('h1')\n",
        "        text = False\n",
        "        if h3:\n",
        "            h3.name = 'h1' #h3 became h1\n",
        "            for span in spans: #iterate through the spans list\n",
        "                if span in h3: #if the span is inside h3\n",
        "                    span.unwrap() #remove the span from the document\n",
        "            text = h3.get_text(strip=True) #extract the text of h2 and assign it to the variable \"text\" removing starting and closing empty space\n",
        "        elif h2:\n",
        "            h2.name = 'h1'\n",
        "            for span in spans:\n",
        "                if span in h2:\n",
        "                    span.unwrap()\n",
        "            text = h2.get_text(strip=True)\n",
        "        elif h1:\n",
        "            for span in spans:\n",
        "                if span in h1:\n",
        "                    span.unwrap()\n",
        "            text = h1.get_text(strip=True)\n",
        "        else: #if there aren't h3, h2 or h1\n",
        "            p = soup.find('p') #find the p tag\n",
        "            if p:\n",
        "                strong = p.find('strong') #find the strong tag inside the p tag\n",
        "                if strong:\n",
        "                    strong.unwrap() #remove the strong tag from the document\n",
        "                p.name = 'h1' #p became h1\n",
        "                for span in spans: #iterate through the spans list\n",
        "                    if span in p:\n",
        "                        span.unwrap() #remove the span from the document\n",
        "                text = p.get_text(strip=True) #extract the text of p and assign it to the variable \"text\" removing starting and closing empty space\n",
        "        if text: #if text is not empty\n",
        "            title.append(text) #append the text to the title variable\n",
        "\n",
        "    else: #if the curator IS Iagnemma, Confalonieri or Cau\n",
        "        h1 = soup.find('h1') #find the h1 tag\n",
        "        title.append(h1.get_text(strip=True)) #extract the text of h1 and assign it to the variable \"title\" removing starting and closing empty space\n",
        "    soup.head.insert(0, title)  #insert the title in the head of the document"
      ],
      "metadata": {
        "id": "NfcO5WTSN8i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questa parte si va a prelevare e pulire il testo del titolo del documento.\n",
        "La funzione Crea un nuovo tag `title` da aggiungere all'head del documento HTML.\n",
        "Poi controlla il valore del curatore nel dizionario metadata.\n",
        "\n",
        "Se il curatore non è uno tra Caterina Iagnemma, Sofia Confalonieri o Maurizio Cau:\n",
        "\n",
        "Cerca i tag `h3`, `h2`, e `h1` nel documento HTML.\n",
        "Se ne trova uno, lo trasforma in `h1` e rimuove eventuali tag `span` contenuti al suo interno, quindi estrae il testo dal tag e lo salva nella variabile `text`.\n",
        "\n",
        "Se non trova nessuno dei tag `h3`, `h2`, o `h1`, cerca un tag `p`:\n",
        "\n",
        "All'interno del tag `p`, rimuove eventuali tag `strong`.\n",
        "Trasforma il `p` in `h1`, rimuove eventuali tag `span` contenuti al suo interno e estrae il testo.\n",
        "Se riesce a estrarre del testo, lo aggiunge al tag `title`.\n",
        "\n",
        "Se il curatore è uno tra Caterina Iagnemma, Sofia Confalonieri o Maurizio Cau, prende semplicemente il testo dal tag `h1` e lo aggiunge al tag `title`.\n",
        "\n",
        "Infine, inserisce il tag `title` creato all'inizio del documento HTML, all'interno del tag `head`."
      ],
      "metadata": {
        "id": "S8Te6gLUWiYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE TRASH AND SCRAPS TAGS\n",
        "def clean_trash_and_scraps(soup):\n",
        "    try:\n",
        "        trash_entities = soup.find_all(attrs={'typeof': 'moro:Trash'}) #find the attribute typeof with moro:Trash or moro:Scraps\n",
        "        scraps_entities = soup.find_all(attrs={'typeof': 'moro:Scraps'})\n",
        "        total = trash_entities + scraps_entities #add the two lists together\n",
        "        if len(total) > 0: #if the list is not empty\n",
        "            for el in total: #iterate through the list\n",
        "                mentions = soup.find_all(attrs={'resource': el['about']})\n",
        "                metas = soup.find_all('meta', attrs={'about': el['about']})\n",
        "                for meta in metas:\n",
        "                    meta.decompose()\n",
        "                for mention in mentions:\n",
        "                    mention.unwrap()\n",
        "    except: None"
      ],
      "metadata": {
        "id": "kSo8qReKaCNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "La funzione `clean_trash_and_scraps` rimuove tag non desiderati dal documento HTML.\n"
      ],
      "metadata": {
        "id": "rXzjwY9KaNiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX CURATOR NOTES AND REMOVE TAGS INSIDE THEM\n",
        "def clean_curator_notes(soup, spans, dataset_ns, metadata):\n",
        "    notes_section = soup.find('ol', id='curatorNotes') #find the curator notes\n",
        "    if notes_section:\n",
        "        notes = notes_section.find_all(attrs={'typeof': 'moro:Footnote'})\n",
        "        if notes: #if the list of notes is not empty\n",
        "            for note in notes:\n",
        "                note['about'] = dataset_ns + f'{metadata[\"ident\"].replace(\"_\", \"\")}/v1/' + note['about'][1:] #add the dataset namespace to the about attribute of the note\n",
        "                note['typeof'] = 'fabio:Comment' #set the typeof attribute of the note to fabio:Comment\n",
        "                h3 = note.find('h3') #find the h3 tag inside the note and remove them\n",
        "                if h3:\n",
        "                    h3.unwrap()\n",
        "                ps = note.find_all('p') #find all p tags inside the note and remove them\n",
        "                if ps:\n",
        "                    for p in ps:\n",
        "                        for span in spans:\n",
        "                            if span in p.find_all('span') and 'bibref' not in span['class']: #if the span is inside p and has no class bibref\n",
        "                                span.unwrap() #remove the span from the document\n",
        "                        p.unwrap() #remove the p tag from the document"
      ],
      "metadata": {
        "id": "LS7cZAPkvtdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "La funzione `clean_curator_notes` pulisce e modifica le note del curatore nel documento HTML, rimuovendo tag non necessari e aggiornando gli attributi."
      ],
      "metadata": {
        "id": "ORhimz4ov4Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX MORO NOTES AND REMOVE TAGS INSIDE THEM\n",
        "def clean_moro_notes(soup, spans, dataset_ns, metadata):\n",
        "    notes_section = soup.find('ol', id='moroNotes') #find all the Moro notes\n",
        "    if notes_section:\n",
        "        notes = notes_section.find_all('li') #find all li tags inside the notes section and store them in the notes variable\n",
        "        if notes:\n",
        "            for note in notes:\n",
        "                note['about'] = dataset_ns + f'{metadata[\"ident\"].replace(\"_\", \"\")}/v1/' + note['about'][1:] #add the dataset namespace to the about attribute of the note\n",
        "                note['typeof'] = 'fabio:Comment' #set the typeof attribute of the note to fabio:Comment\n",
        "                del note['data-toggle'] #delete attributes data-toggle, data-placement, title, data-original-title e class.\n",
        "                del note['data-placement']\n",
        "                del note['title']\n",
        "                del note['data-original-title']\n",
        "                del note['class']\n",
        "                ps = note.find_all('p') #find all p tags inside the note\n",
        "                if ps:\n",
        "                    for p in ps:\n",
        "                        for span in spans:\n",
        "                            if span in p.find_all('span') and 'bibref' not in span['class']: #if the span is inside p and has no class bibref\n",
        "                                span.unwrap() #remove the span from the document\n",
        "                        p.unwrap() #remove the p tag from the document\n",
        "\n",
        "                for child in note.children: #for every children of the note, if it's a string, remove the \"[\", \"]\" charcaters from the text\n",
        "                    if child.string:\n",
        "                        fixed_text = child.string.replace('[', '').replace(']', '')\n",
        "                        child.string.replace_with(fixed_text)\n",
        "\n",
        "        del notes_section['data-alert'] #delete the data-alert attribute from the \"Moro\" section notes"
      ],
      "metadata": {
        "id": "lvLHXmpyv8er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funzione `clean_moro_notes` pulisce e modifica le note taggate nel testo tramite KwicKwocKwac presenti nel documento HTML, rimuovendo tag non necessari e aggiornando gli attributi."
      ],
      "metadata": {
        "id": "_JoYODPWw5n5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX PARAGRAPHS\n",
        "def clean_paragraphs(soup):\n",
        "    index = 1\n",
        "    for p in soup.find_all('p'): #find all the paragraphs\n",
        "        if (len(p.get_text(strip=True)) <= 40 and not p.get_text(strip=True).endswith(':')) or p.get_text(strip=True).startswith('-') or p.get_text(strip=True).startswith('–') or re.match(r'^\\d\\).+', p.get_text(strip=True)) or re.match(r'(^IX|IV|V?I{0,3})\\).+', p.get_text(strip=True)) or re.match(r'^\\D\\).+', p.get_text(strip=True)):\n",
        "            continue\n",
        "        else:\n",
        "            try:\n",
        "                p['id'] = f'p-{index}'\n",
        "                p['class'] = 'paragraph'\n",
        "                p['data-counter'] = index\n",
        "                index += 1\n",
        "            except: None"
      ],
      "metadata": {
        "id": "qGA6a2ElzcD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funzione `clean_paragraphs` pulisce e assegna attributi ai paragrafi `p` di un documento HTML.\n",
        "\n",
        "For each paragraph, check if:\n",
        "\n",
        "- The text length is less than or equal to 40 characters and does not end with a : or\n",
        "- The text starts with - or – or\n",
        "- The text matches a numbered list pattern (e.g., \"1)\", \"IX)\", \"IV)\", etc.) or alphabetic list pattern (e.g., \"A)\", \"B)\", etc.).\n",
        "\n",
        "If none of the conditions are true:\n",
        "\n",
        "- Try to assign a unique id to the paragraph in the format p-{index}.\n",
        "- Assign the class paragraph.\n",
        "- Add a data-counter attribute with the value of the index counter.\n",
        "- Increment the index counter by 1."
      ],
      "metadata": {
        "id": "vhFEX6QC4bEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX MENTIONS AND MENTIONED ENTITIES\n",
        "def clean_entities(soup, typeof, dataset_ns, metadata):\n",
        "    category = soup.find_all(attrs={'typeof': typeof}) # Find all entities with the specified 'typeof' attribute\n",
        "    for entity in category: # Iterate over each found entity and change the 'typeof' attribute based on the entity type\n",
        "        if typeof == 'moro:Person':\n",
        "            entity['typeof'] = 'foaf:Person'\n",
        "        elif typeof == 'moro:Organization':\n",
        "            entity['typeof'] = 'foaf:Organization'\n",
        "        elif typeof == 'moro:Place':\n",
        "            entity['typeof'] = 'dcterms:Location'\n",
        "        elif typeof == 'moro:Quotation':\n",
        "            entity['typeof'] = 'doco:TextChunk'\n",
        "            entity['about'] = dataset_ns + f'{metadata[\"ident\"].replace(\"_\", \"\")}/v1/' + entity['about'][1:] # Update the 'about' attribute for quotations\n",
        "            del entity['data-label'] # Remove the 'data-label' attribute\n",
        "            if entity.find('span', 'quote-text'):\n",
        "                entity.find('span', 'quote-text').unwrap() # Unwrap 'span' tags with the class 'quote-text' if they exist\n",
        "        # Find all mentions, meta information, notes, and bibliographic references related to the entity\n",
        "        mentions = soup.find_all(attrs={'property': 'dcterms:references', 'resource': entity['about']})\n",
        "        metas = soup.find_all(attrs={'about': entity['about']})\n",
        "        notes = soup.find_all(attrs={'property': 'dcterms:creator', 'resource': entity['about']})\n",
        "        bibrefs = soup.find_all(attrs={'property': 'biro:references', 'resource': entity['about']})\n",
        "        if metas:\n",
        "            for meta in metas:\n",
        "                if typeof != 'fabio:Expression':\n",
        "                    if meta.has_attr('content') and meta['property'] == 'rdfs:label':  # Clean and update the 'content' attribute for 'rdfs:label'\n",
        "\n",
        "                        meta['content'] = meta['content'].strip()\n",
        "\n",
        "                        entity['about'] = dataset_ns + unidecode.unidecode(re.sub(r'[ \\'’]', '-', meta['content'].strip().replace('.', '').replace('\"', '').replace('?', '').replace(',', '').replace(':', ''))).lower()\n",
        "\n",
        "                    # Change 'dcterms:relation' to 'owl:sameAs' and update 'about' attribute using external API data\n",
        "                    elif meta.has_attr('property') and meta['property'] == 'dcterms:relation':\n",
        "                        meta['property'] = 'owl:sameAs'\n",
        "                        wikidata_entity = meta['resource'].split('/')[-1]\n",
        "                        entity_dict = get_entity_dict_from_api(wikidata_entity)\n",
        "                        item = WikidataItem(entity_dict)\n",
        "\n",
        "                        if item.get_label(lang='it'):\n",
        "                            entity['about'] = dataset_ns + unidecode.unidecode(re.sub(r'[ \\'’]', '-', item.get_label(lang='it').strip().replace('.', '').replace('\"', '').replace('?', '').replace(',', '').replace(':', ''))).lower()\n",
        "                        else:\n",
        "                            entity['about'] = dataset_ns + unidecode.unidecode(re.sub(r'[ \\'’]', '-', item.get_label(lang='en').strip().replace('.', '').replace('\"', '').replace('?', '').replace(',', '').replace(':', ''))).lower()\n",
        "\n",
        "                    # Clean and update 'moro:altLabel'\n",
        "                    elif meta.has_attr('property') and meta['property'] == 'moro:altLabel':\n",
        "\n",
        "                        meta['content'] = meta['content'].strip()\n",
        "\n",
        "                        if typeof == 'moro:Person' or typeof == 'foaf:Person':\n",
        "                            meta['property'] = 'skos:prefLabel'\n",
        "                            # Update the 'about' attribute for complex titles\n",
        "                            if ', ' not in meta['content'] and (' dell\\'' not in meta['content'] or ' di ' not in meta['content']):\n",
        "                                meta['content'] = ', '.join(re.findall(r'[A-Z][a-z]*', unidecode.unidecode(meta['content'])))\n",
        "…\n",
        "                                if year:\n",
        "                                    entity['about'] = dataset_ns + unidecode.unidecode(re.sub(r'[°\\[\\]\":,\\?“”!\\\\\\/.()«»]', '', split_content[0] + title_init + '-' + year.group())).strip().replace(' - ', '-').replace('\\'', '-').replace(' ', '-').lower()\n",
        "                                else:\n",
        "                                    entity['about'] = dataset_ns + unidecode.unidecode(re.sub(r'[°\\[\\]\":,\\?“”!\\\\\\/.()«»]', '', split_content[0] + title_init)).strip().replace(' - ', '-').replace('\\'', '-').replace(' ', '-').lower()\n",
        "                            else:\n",
        "                                entity['about'] = dataset_ns + unidecode.unidecode(re.sub(r'[°\\[\\]\":,\\?“”!\\\\\\/.()«»]', '', meta['content'])).strip().replace(' - ', '-').replace('\\'', '-').replace(' ', '-').lower()\n",
        "                        except: None\n",
        "\n",
        "            # Update the 'about' attribute in all meta tags\n",
        "            for meta in metas:\n",
        "                meta['about'] = entity['about']\n",
        "\n",
        "        # Update mentions\n",
        "        if mentions:\n",
        "            for mention in mentions:\n",
        "                mention['about'] = dataset_ns + f'{metadata[\"ident\"].replace(\"_\", \"\")}/v1/' + mention['about'][1:]\n",
        "                mention['typeof'] = 'deo:Reference'\n",
        "                mention['resource'] = entity['about']\n",
        "                if 'scraps' in mention['class']:\n",
        "                    mention['class'].replace('scraps', '')\n",
        "\n",
        "        # Update notes\n",
        "        if notes:\n",
        "            for note in notes:\n",
        "                entity['about'] = dataset_ns + unidecode.unidecode(re.sub(r'[ \\'’]', '-', meta['content'].strip().replace('.', ''))).lower()\n",
        "                note['resource'] = entity['about']\n",
        "\n",
        "        # Update bibliographic references\n",
        "        if bibrefs:\n",
        "            for bibref in bibrefs:\n",
        "                bibref['about'] = dataset_ns + f'{metadata[\"ident\"].replace(\"_\", \"\")}/v1/' + bibref['about'][1:]\n",
        "                bibref['typeof'] = 'biro:BibliographicReference'\n",
        "                bibref['resource'] = entity['about']\n",
        "                if 'scraps' in bibref['class']:\n",
        "                    del bibref['class'][1]"
      ],
      "metadata": {
        "id": "QmSAzx2K5mRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questa funzione `clean_entities` si oocupa di processare le entità taggate dai ricercatori sui testi tramite la piattaforma KwicKwicKwac.\n",
        "\n",
        "Le entità sono: Person, Organization, Location, Quotation.\n",
        "\n",
        "Inizialmente modifica gli attributi del typeof passando dall'ontologia moro: a quelle più appropriate in base alla tipologià dell'entità.\n",
        "\n",
        "Se l'entità è una citazione, viene aggiornato l'attributo about e vengono rimossi i tag non necessari.\n",
        "\n",
        "La funzione cerca riferimenti, meta informazioni, note e riferimenti bibliografici correlati all'entità e li aggiorna di conseguenza.\n",
        "I metadati come rdfs:label, dcterms:relation e moro:altLabel vengono puliti e aggiornati. Viene utilizzata un'API esterna per ottenere ulteriori informazioni nel caso dell'entità Wikidata.\n",
        "\n",
        "Anche i riferimenti bibliografici vengono aggiornati con nuovi attributi e classi, integrando informazioni prese da MongoDB.\n"
      ],
      "metadata": {
        "id": "rA8I4XWSLiVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX NAMESPACES\n",
        "def clean_namespaces(soup):\n",
        "    # Find different types of elements in the head and body sections\n",
        "    people = soup.head.find(attrs={'typeof': 'foaf:Person'})\n",
        "    orgs = soup.head.find(attrs={'typeof': 'foaf:Organization'})\n",
        "    places = soup.head.find(attrs={'typeof': 'dcterms:Location'})\n",
        "    exps = soup.head.find(attrs={'typeof': 'fabio:Expression'})\n",
        "    mentions = soup.body.find(attrs={'typeof': 'deo:Reference'})\n",
        "    bibrefs = soup.body.find(attrs={'typeof': 'biro:BibliographicReference'})\n",
        "    quotations = soup.body.find(attrs={'typeof': 'doco:TextChunk'})\n",
        "    notes = soup.body.find(attrs={'typeof': 'fabio:Comment'})\n",
        "\n",
        "    # If any relevant elements are found in the head section, set up the namespace prefixes\n",
        "    if people or orgs or places or exps:\n",
        "        soup.head['prefix'] = ''\n",
        "        namespace = 'rdfs: http://www.w3.org/2000/01/rdf-schema# '\n",
        "        soup.head['prefix'] += namespace\n",
        "        # Add the 'owl' namespace if there is the property 'sameAs'\n",
        "        if soup.find(attrs={'property': 'owl:sameAs'}):\n",
        "            namespace = 'owl: http://www.w3.org/2002/07/owl# '\n",
        "            soup.head['prefix'] += namespace\n",
        "        # Add the 'foaf' namespace if there are people or organizations + the wikidata property for the wikidata id\n",
        "        if people or orgs:\n",
        "            namespace = 'foaf: http://xmlns.com/foaf/0.1/ '\n",
        "            soup.head['prefix'] += namespace\n",
        "            if soup.find(attrs={'property': 'wdt:P1986'}):\n",
        "                soup.head['prefix'] += 'wdt: http://www.wikidata.org/prop/direct/ '\n",
        "        # Add the 'fabio' and 'dcterms' namespaces based on the presence of places and expressions\n",
        "        if places and exps:\n",
        "            namespace = 'fabio: http://purl.org/spar/fabio/ '\n",
        "            if 'dcterms' not in soup.head['prefix']:\n",
        "                namespace2 = 'dcterms: http://purl.org/dc/terms/ '\n",
        "                soup.head['prefix'] += namespace + namespace2\n",
        "            else:\n",
        "                soup.head['prefix'] += namespace\n",
        "        elif places and not exps:\n",
        "            if 'dcterms' not in soup.head['prefix']:\n",
        "                namespace = 'dcterms: http://purl.org/dc/terms/ '\n",
        "                soup.head['prefix'] += namespace\n",
        "        elif exps and not places:\n",
        "            namespace = 'fabio: http://purl.org/spar/fabio/ '\n",
        "            if 'dcterms' not in soup.head['prefix']:\n",
        "                namespace2 = 'dcterms: http://purl.org/dc/terms/ '\n",
        "                soup.head['prefix'] += namespace + namespace2\n",
        "            else:\n",
        "                soup.head['prefix'] += namespace\n",
        "        # Add the 'skos' namespace if there is 'prefLabel'\n",
        "        if soup.find(attrs={'property': 'skos:prefLabel'}):\n",
        "            namespace = 'skos: http://www.w3.org/2004/02/skos/core# '\n",
        "            soup.head['prefix'] += namespace\n",
        "        # Remove the trailing space\n",
        "        soup.head['prefix'] = soup.head['prefix'][:-1]\n",
        "\n",
        "    # If any relevant elements are found in the body section, set up the namespace prefixes\n",
        "    if mentions or notes or bibrefs or quotations:\n",
        "        soup.body['prefix'] = ''\n",
        "        if mentions and notes:\n",
        "            namespace = 'deo: http://purl.org/spar/deo/ '\n",
        "            namespace2 = 'dcterms: http://purl.org/dc/terms/ '\n",
        "            namespace3 = 'fabio: http://purl.org/spar/fabio/ '\n",
        "            soup.body['prefix'] += namespace + namespace2 + namespace3\n",
        "        elif mentions and not notes:\n",
        "            namespace = 'deo: http://purl.org/spar/deo/ '\n",
        "            namespace2 = 'dcterms: http://purl.org/dc/terms/ '\n",
        "            soup.body['prefix'] += namespace + namespace2\n",
        "        elif notes and not mentions:\n",
        "            namespace = 'fabio: http://purl.org/spar/fabio/ '\n",
        "            namespace2 = 'dcterms: http://purl.org/dc/terms/ '\n",
        "            soup.body['prefix'] += namespace + namespace2\n",
        "        # Add the 'biro' namespace if there are bibrefs and 'doco' namespace if there are quotations\n",
        "        if bibrefs:\n",
        "            namespace = 'biro: http://purl.org/spar/biro/ '\n",
        "            soup.body['prefix'] += namespace\n",
        "        if quotations:\n",
        "            namespace = 'doco: http://purl.org/spar/doco/ '\n",
        "            soup.body['prefix'] += namespace\n",
        "        soup.body['prefix'] = soup.body['prefix'][:-1]\n"
      ],
      "metadata": {
        "id": "WpOf5bODSM7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funzione `clean_namespaces` cerca diversi tipi di elementi (persone, organizzazioni, luoghi, riferimenti bibliografici e citazioni) nel documento HTML.\n",
        "\n",
        "Se vengono trovati elementi rilevanti nella sezione `head` del documento, viene inizializzata la stringa dei prefissi con i relativi namespace (rdfs, owl, foaf, wdt, fabio, dcterms e skos).\n",
        "\n",
        "Se vengono trovati elementi rilevanti nella sezione `body` del documento (citazioni, riferimenti bibliografici e note), viene inizializzata la stringa dei prefissi con i relativi namespace (deo, fabio, dcterms, biro e doco).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7D5Q8z_tVvv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN FUNCTION TO REFACTOR HTML-RDFA CODE\n",
        "def recast(html_path, metadata):\n",
        "    with open(html_path, encoding='utf-8') as fp:\n",
        "        old_soup = BeautifulSoup(fp, 'html.parser')\n",
        "\n",
        "    # CLOSE FILE\n",
        "    fp.close()\n",
        "\n",
        "    # SET NEW SOUP\n",
        "    soup = BeautifulSoup('<!DOCTYPE html><html><head></head><body></body></html>', 'html.parser')\n",
        "\n",
        "    # SET DATASET NAMESPACE\n",
        "    dataset_ns = 'https://w3id.org/moro/enoam/data/'\n",
        "\n",
        "    # FIND ALL SPANS\n",
        "    spans = old_soup.find_all('span')\n",
        "\n",
        "    # LAUNCH ALL FUNCTIONS\n",
        "    convert_head_body(old_soup, soup)\n",
        "    clean_title(soup, spans, metadata)\n",
        "    insert_seo_meta(soup, metadata)\n",
        "    clean_empty_head(soup)\n",
        "    clean_empty_body(soup)\n",
        "    clean_trash_and_scraps(soup)\n",
        "    clean_curator_notes(soup, spans, dataset_ns, metadata)\n",
        "    clean_moro_notes(soup, spans, dataset_ns, metadata)\n",
        "    clean_entities(soup, 'foaf:Person', dataset_ns, metadata)\n",
        "    clean_entities(soup, 'moro:Person', dataset_ns, metadata)\n",
        "    clean_entities(soup, 'moro:Organization', dataset_ns, metadata)\n",
        "    clean_entities(soup, 'moro:Place', dataset_ns, metadata)\n",
        "    clean_entities(soup, 'fabio:Expression', dataset_ns, metadata)\n",
        "    clean_entities(soup, 'moro:Quotation', dataset_ns, metadata)\n",
        "    clean_paragraphs(soup)\n",
        "    clean_namespaces(soup)\n",
        "\n",
        "\n",
        "    # RETURN HTML AS STRING\n",
        "    return str(soup)"
      ],
      "metadata": {
        "id": "7EL3rncsXVaK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}